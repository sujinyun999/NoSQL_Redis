# 6.1 복제 & 분산 시스템 개요

레디스는 **스케일 아웃** 기능인 파티셔닝을 제공

cf. **스케일 업** : 하나의 서버 내에서 시스템 자원을 최대한 확장하여 데이터를 저장, 관리하는 기술

**스케일 아웃** : 하나의 논리적인 테이블을 수대~수십대의 서버로 분산 저장, 데이터를 분리 저장하는 목적은 동일하지만 분산 방법은 완전 다름



> 데이터 분산 저장 목적
> 
> 1. **자원 공유** : 하나의 서버에서 사용가능한 시스템 자원이 제한적, 한번에 많은 데이터를 다룰경우 과부화 발생 가능성
> 
> 2. **성능 향상** : 연속적으로 발생하는 데이터 처리에 한계가오면, 일시적인 대기상태 발생, 그리고 시스템 전체의 성능 지연 => 로드밸런싱으로 다른서버에 연속적으로 데이터 저장
> 
> 3. **안정성** : 특정 서버에 장애가 발생했을경우 데이터 복제로 유실방지

레디스에서의 데이터 복제, 분산처리 => 마스터-슬레이브, 마스터-슬레이브-센티널, 파티션 클러스트 기능

## 6.1.1 Partition 유형


1. **Range Partition** : Redis서버에 저장되는 key값을 기준으로 특정범위 데이터를 특정 파티션 서버로 분산 저장

2. **Hash Partition** : Range partitioning의 장점은 사용자가 지정한 서버로 특정 범위값 저장이 가능하다는 것, 단점은 각 샤드 서버에 저장되는 데이터 양에따라 분산율이 떨어지는 경우가 발생할 수 있다는 것, 이를 보완해 Hash algorithm에 의해 골고루 분산


## 6.1.2 Partition 구현 방법

1. **client side partitioning** : 데이터를 읽고 쓰는 클라이언트에서 해당 서버 직접 지정 및 처리, redis서버를 이용하는 대부분의 시스템 환경에서 보편적으로 사용하는 방법
  
     사용자의 능력에 따라 엄청 효과적일수도, 불편할 수도 있다, 쉽지않음

2. **proxy assisted partitioning** : 클라이언트는 proxy protocol로 구축된 proxy server를 통해 쓰고 읽기 작업에 대한 요청 수행, 결과 전달받음
  
    분산서버 이외에 프록시서버가 추가로 필요, 이때 프록시 서버는 현재 분산서버의 모든 상태 정보를 수집 및 저장해주고, 사용자가 대량의 데이터에 대한 저장을 요구하는 경우 가장 적절한 분산 서버를 찾아서 데이터 저장해줌 / 사용자가 특정 데이터를 검색할 경우 해당 데이터가 저장되어 있는 서버 정보를 분석한 후 데이터를 검색, 결과 리턴 => 별도 분리된 서버에 구축 권장

3. **query routing**: 읽고 쓰기 작업 수행 -> 임의의 서버로 전달, 데이터를 참조할 수 없는 경우 올바른 노드로 redirection(자동전환)되어 실행

    해시파티션으로 데이터 분산 저장, 특정 분산서버에 장애가 발생한 경우 사용가능한 slave서버를 통해 지속적인 읽기 작업 가능, 사용가능한 서버로 자동 전환해줌

#### *Redis 파티션의 단점 : 
   1. 레디스 서버 환경에서 파티셔닝 기능을 이용한 분산처리 기술의 사용은 적극적으로 권장하지 않는다,,,,,,,,,,,,,,,,,

   2. 새로운 노드를 추가, 기존 노드를 실시간으로 제거하는 작업은 하나의 큰 파티션 영역을 새롭게 분할, 합병하는 작업이 빈번하게 요구되는데 이 경우 rdb와 aof파일을 백업하고 이전해야하기 때문에 이 작업들은 결코 쉬운 방법이 아님

   3. 런타임에서 노드추가, 제거 작업이 수행되는 단계에서 전체 서버의 균형을 맞추기 위한 리밸런싱 작업을 수행해야하는데 이를 처리하다 보면 성능 지연 문제 등 기술적 한계가 발생할 수 있음


# 6.2 Master & slave & sentinel


## 6.2.1  시스템 설정

1. **마스터-슬레이브 복제시스템** : 사용자 데이터를 실시간으로 처리(입력, 수정, 삭제, 조회)할 수 있는 마스터 서버 1대에 대해 슬레이브 서버는 마스터 서버의 데이터가 실시간으로 복제됨, 슬레이브는 서버 마스터 서버에의해서 만 쓰기 작업을 수행할 수 있고 사용자는 읽기작업만 수행할 수 있음


**마스터 서버에 장애가 발생하는 경우**
      
   슬레이브 서버 -> 마스터 서버 (자동전환X)
      
   사용자는 슬레이브 서버에 복제된 데이터를 이용해 마스터 서버 복구 가능
      
   슬레이브 서버에 대해서는 지속적인 읽기 작업 수행 가능(쓰기 작업은 불가능)
   
2. **마스터-슬레이브-센티널 복제시스템** : 센티널 서버는 일상적인 업무 환경에서는 마스터 서버와 슬레이브 서버를 지속적으로 모니터링하다가 마스터 서버에 장애가 감지된 경우 슬레이브 서버를 즉시 마스터 서버로 자동 전환시켜 데이터 유실이 발생하지 않도록 FAIL-OVER해줌

    센티널 서버에는 사용자 데이터가 저장되지 않으며 오직 마스터 서버와 슬레이브 서버릐 장애상태만을 모니터링하는 역할 수행! -> 좋은 하드웨어 필요없음
    
    원본 센티널 서버에 장애가 발생할경우 복제 센티널 서버 추가 구성 가능
    
    최소 1~3대 추천!
    
    > 센티널 서버 : 
    > 
    > 1. 원본 데이터 서버에 예기치 못한 장애가 발생하는 경우 데이터가 유실될 수 있음, 이와 같은 상황이 발생하더라도 언제든지 서비스를 수행하기 위해 슬레이브 서버로 자동 FAIL-OVER해주는 기능 필요!
    > 
    > 2. 마스터 서버와 슬레이브 서버가 어떤 상태인지를 거의 실시간으로 HEARTBEAT를 통해 감시하고 관련 정보를 제공
    > 
    > 3. 데이터 서버에 어떤 장애가 발생하는 경우 문자, 이메일 등을 통해 관련 상태 정보 사용자에게 전달 가능


## 6.2.2 장애처리 방법

1. 센티널 서버는 매 1초마다 heartbeat를 통해 master서버와 slave 서버가 작동중인지 여부를 확인

2. **일정 time out동안 더이상 응답이 없음**을 장애로 인식하는 경우 = 주관적 다운 / +sdown (subjectively down)

  cf.  주관적 다운은 하나의 센티널 서버가 장애상태를 인지한경우에 대한건데, 만약 센티널 서버가 여러개라면?
  
  : 모든 센티널 서버가 장애 상태를 인지한경우 = 객관적 다운 / +odown (objectively down)
  
  센티널 서버는 마스터 서버가 다운된 경우 다른 센티널 서버와 함께 전체 정족수(Quorum)를 확인한 다음, 이에 미치지 못한 경우 최종적으로 실제 다운되었다고 판단
  
  > Quorum?
  > 
  >  : 분산 시스템에서 작업을 수행하기 위해 분산 트랜잭션이 획득해야하는 최소 투표 수, 쿼럼 기반 기술은 분산 시스템에서 일관된 작업을 시행하기 위해 구현된다.
  
3. **주관적, 객관적 다운이 최종 확인되면** 장애조치 작업을 단계별 수행

4. 여러 센티널서버가 있을경우, 센티널 리더 선정

5. 리더 센티널 서버는 장애가 발생한 MASTER서버를 대신할 slave서버 선정

6. 선정된 slave서버는 master서버로 승격

7. 남은 slave서버가 새로운 master서버 모니터링

8. 센티널 서버 정보 갱신, 장애 복구 작업 종료

여러대의 센티널 서버로 구축되어있는 시스템 환경에서 장애가 발생했을때, 센티널 서버의 리더를 결정해야댐 -> 이때 불필요한 시간낭비를 최소화하고 서버 관리자의 의도대로 장애조치를 수행해야하는 경우 우선순위 설정이 가능하다고 합니다! 여러대의 slave서버 중에 master서버를 새로 선정하는 경우도 마찬가지!

# 6.3 부분동기화

master 서버는 항상 read/write가 가능하지만 slave서버는 기본적으로 read-only. 

이때, master서버에서 장애가 발생하는 경우 데이터 유실이 발생할 수 밖에 없고, 이를 방지하기 위해 복제 서버에는 실시간 전체 동기화 작업이 수행된다

slave서버는 이전 복제 데이터는 제거하고, 새로운 master서버로부터 모든 데이터를 full sync 동기화


# 6.4 Redis cluster 구축 및 운영

분산 시스템 : 하나의 테이블에 저장되는 데이터를 2개 이상의 서버로 동시에 분산 저장

- 파티셔닝을 통해 데이터를 분산처리 할 경우, 어쩌다 특정 서버에 장애가 발생하게 되었을때 데이터 유실 가능성,,!!

-> 분산 서버마다 복제 서버를 함께 구축 운영

-> 파티셔닝 시스템 + 복제시스템 => Redis 클러스터

**Redis cluster시스템을 구축하는 방법**

1. cluster명령어를 이용해 사용자가 직접 수동 설정

    - 사용자가 직접 물리적 설계
    
    - 최적화된 cluster서버 환경 구축 가능
    
    - 장애 발생시 관리자의 의도에 따라 대응 가능


2. redis-trib.rb 유틸리티 이용, 자동 설정

    - 자동화된 알고리즘을 통한 cluster서버 구축

    - cluster구축 및 운용 관리에 대한 기술적 이해가 부족해도 구현 가능

    - 장애 발생시 자동화 알고리즘을 통해 시스템 재구성 수행


> Redis 클러스터의 특징
>  1. database 0번만 사용가능
>  2. mset명령어 사용 불가 (mset : 여러개 필드 한번에 저장)
>  3. hash-tag를 통해 데이터 표현 가능
>  4. 센티널 서버는 없어도됨
>  5. 최소 3대의 master서버 요구(안정성)
>  




## Redis Cluster 구성

![image](https://user-images.githubusercontent.com/69068083/119209719-43a28900-bae3-11eb-806f-fba7965d95b3.png)


모든 노드는 서로서로 연결된 full mesh구조

가십 프로토콜을 이용한 통신



### Sharding

**샤딩이란?**
![image](https://user-images.githubusercontent.com/69068083/119210006-fcb59300-bae4-11eb-9f79-2d310996d963.png)


어플리케이션으로부터 들어오는 모든 데이터는 해시 슬롯에 저장된다. 레디스 클러스터는 총 16384개의 슬롯을 가지며, 마스터 노드는 슬롯을 나누어 저장. 

마스터 노드가 세개일 때 아래처럼 해시슬롯이 분배될 수 있음.


![image](https://user-images.githubusercontent.com/69068083/119209781-a8f67a00-bae3-11eb-85c8-a9979ed833c9.png)


해시슬롯은 마스터 노드 내에서 자유롭게 옮겨질 수 있으며, 이를 위한 다운타임은 필요하지 않다. 

-> 새로운 노드를 추가하거나 기존 노드를 삭제할 때에는 해시슬롯을 이동시키기만 하면 됨! -> 쉬운 확장 가능


### Failover

레디스 클러스터에서 마스터 노드가 다운되면?

: 연결된 복제 노드를 마스터로 승격시키는 Fail-over

cf. Sentinel 구조에서는 Sentinel 프로세스가 노드들을 감시했지만, 레디스 클러스터 구조에서는 모든 노드가 서로서로 감시한다


ex. 가용성이 중요한 서비스에서 레디스 클러스터 구성을 이용할 때 노드 하나를 더 추가할 수 있는 여유가 있다면?


![image](https://user-images.githubusercontent.com/69068083/119209858-11ddf200-bae4-11eb-8f81-99e10c176dc2.png)





### Client Redirection

**어플리케이션이 분할된 데이터를 저장하는법**

: 어플리케이션은 샤딩을 생각하지 않고 아무 데로나 던져도 라이브러리와 레디스 서버가 알아서 다 해준다!



![image](https://user-images.githubusercontent.com/69068083/119209869-2ae6a300-bae4-11eb-9161-e69898a4c8ea.png)

![image](https://user-images.githubusercontent.com/69068083/119209878-3508a180-bae4-11eb-9da2-39cd3aa73bd0.png)

![image](https://user-images.githubusercontent.com/69068083/119209884-405bcd00-bae4-11eb-9bfe-99c6177a74a2.png)


![image](https://user-images.githubusercontent.com/69068083/119209891-49e53500-bae4-11eb-951c-84eeb37274f3.png)












출처 : 
https://meetup.toast.com/posts/226

https://nesoy.github.io/articles/2018-05/Database-Shard


